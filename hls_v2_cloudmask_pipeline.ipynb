{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be689edd-cbdd-4b3a-a165-ce588c21b2c7",
   "metadata": {},
   "source": [
    "# HLS Version 2 Cloud Mask query, download and chipping pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3928bc-bf73-4843-b515-b1b768fda533",
   "metadata": {},
   "source": [
    "### More information on how functions work can be found in the hls_v2_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a906205-e792-4167-9879-b0c59f4f0953",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248bdc55-5416-4c0f-b76c-6bbb5ff59df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import json\n",
    "import shapely\n",
    "import shapely.geometry\n",
    "import xarray\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import os\n",
    "import fiona\n",
    "import urllib.request as urlreq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import xmltodict\n",
    "import shutil\n",
    "import datetime\n",
    "import boto3\n",
    "import pyproj\n",
    "\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "from pystac_client import Client \n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio import Affine\n",
    "from rasterio.crs import CRS\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import Popen, PIPE\n",
    "from tqdm import tqdm\n",
    "from netrc import netrc\n",
    "from subprocess import Popen\n",
    "from platform import system\n",
    "from getpass import getpass\n",
    "from rasterio.session import AWSSession\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f97211-2dcc-45a8-bf04-255679bc328c",
   "metadata": {},
   "source": [
    "## Setting folder pathes and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3966fbc-2da6-4504-9eeb-707831a40aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START OPTIONS #####\n",
    "\n",
    "## Threshold of cloud cover percentage. \n",
    "## 30 pct to 80 pct is used to get various could cover pct at chip level\n",
    "cloud_lower = 30\n",
    "cloud_upper = 80\n",
    "\n",
    "# Root paths\n",
    "root_path = \"/data/cloudMask/\"\n",
    "req_path = \"/home/data/fmask/\"\n",
    "extra_files = \"/data/requirements/\"\n",
    "json_path = \"/home/data/\"\n",
    "\n",
    "# Paths to necessary files\n",
    "chip_file =  json_path + \"chip_bbox_task_3_5070.geojson\"\n",
    "query_file = json_path + \"chip_bbox_task_3.geojson\"\n",
    "chip_csv = req_path + \"chip_tracker.csv\"\n",
    "kml_file = extra_files + 'sentinel_tile_grid.kml'\n",
    "\n",
    "## Saving paths\n",
    "## Manually creating these folders is recommended before running the pipeline\n",
    "chip_dir = root_path + 'cloud_mask/'\n",
    "tile_dir = root_path + 'tiles_fmask/'\n",
    "chip_fmask_dir = root_path + \"cloud_mask/\"\n",
    "\n",
    "#####  END OPTIONS  #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e884a-d995-48c1-95c5-db1643b65a3b",
   "metadata": {},
   "source": [
    "## Read in csvs and jsons from saved file (Only run when need to load from previously query results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c069f6b-9a39-423e-880d-c05eb00a8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all saved dataframes and json\n",
    "chip_df = pd.read_csv(req_path + \"chip_df.csv\")\n",
    "with open(json_path + \"chip_ids.json\", 'r') as f:\n",
    "    chip_ids = json.load(f)\n",
    "track_df = pd.read_csv(req_path + \"track_df.csv\")\n",
    "with open(query_file, \"r\") as file:\n",
    "    chips = json.load(file)\n",
    "selected_tiles = pd.read_csv(req_path + \"selected_tiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70661fd8-19e9-44e4-a194-efd1f323ceeb",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed51a15-e247-47cd-b6a4-977756b4178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading chips bounding boxes from geojson\n",
    "with open(query_file, \"r\") as file:\n",
    "    chips = json.load(file)\n",
    "\n",
    "# Create lists about chip information to find tiles corresponding to it later\n",
    "chip_ids = []\n",
    "chip_x = []\n",
    "chip_y = []\n",
    "for item in chips['features']:\n",
    "    chip_ids.append(item['properties']['id'])\n",
    "    chip_x.append(item['properties']['center'][0])\n",
    "    chip_y.append(item['properties']['center'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836e6c8-e23d-4531-aa00-333f6c3b0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the chip_ids for chipping uses\n",
    "with open(\"/home/data/chip_ids.json\", \"w\") as f:\n",
    "    json.dump(chip_ids, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6642fe-7b11-434f-b1b9-9bb1bacb3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sentinel kml file\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "tile_src = geopandas.read_file(kml_file, driver='KML')\n",
    "\n",
    "# Create table containing information about sentinel tiles\n",
    "tile_name = []\n",
    "tile_x = []\n",
    "tile_y = []\n",
    "for tile_ind in range(tile_src.shape[0]):\n",
    "    tile_name.append(tile_src.iloc[tile_ind].Name)\n",
    "    tile_x.append(tile_src.iloc[tile_ind].geometry.centroid.x)\n",
    "    tile_y.append(tile_src.iloc[tile_ind].geometry.centroid.y)\n",
    "tile_name = np.array(tile_name)\n",
    "tile_x = np.array(tile_x)\n",
    "tile_y = np.array(tile_y)\n",
    "tile_src = pd.concat([tile_src, tile_src.bounds], axis = 1)\n",
    "tile_src.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc8d89-fc24-4bba-a459-f78ae8de8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tile(x,y):\n",
    "# Identify closest tile\n",
    "    s = (tile_x - x)**2+(tile_y - y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    return(tname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec3c74-4751-4e0e-9d27-6a9d6acd3f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign each chip a tile by using the find_tile function\n",
    "chip_df = pd.DataFrame({\"chip_id\" : chip_ids, \"chip_x\" : chip_x, \"chip_y\" : chip_y})\n",
    "chip_df['tile'] = chip_df.apply(lambda row : find_tile(row['chip_x'], row['chip_y']), axis = 1)\n",
    "chip_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e225b-501a-4775-8a4f-5d10a4e04f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv for later uses\n",
    "chip_df.to_csv(req_path + \"chip_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833df03-02b7-40da-8257-d8419fb340b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = chip_df.tile.unique().tolist()\n",
    "tiles[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7af0f-1c99-47c4-ae59-c1e0f72cf0f1",
   "metadata": {},
   "source": [
    "## Querying tile links based on geometry of chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81d838-473d-46b0-ac25-cefae3cabb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "catalog = Client.open(f'{STAC_URL}/LPCLOUD/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80adeec-2bea-4796-8b2b-7110e9f059bb",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query based on the cloud pct bounds\n",
    "tile_list = []\n",
    "print(f\"There are a total of {len(tiles)} tiles\")\n",
    "tile_iter = 0\n",
    "for current_tile in tiles:\n",
    "\n",
    "    chip_df_filt = chip_df.loc[chip_df.tile == current_tile]#.reset_index()\n",
    "    first_chip_id = chip_df_filt.chip_id.iloc[0]\n",
    "    first_chip_index_in_json = chip_ids.index(first_chip_id)\n",
    "    roi = chips['features'][first_chip_index_in_json]['geometry']\n",
    "\n",
    "    search = catalog.search(\n",
    "        collections = ['HLSS30.v2.0'],\n",
    "        intersects = roi,\n",
    "        datetime = '2022-03-01/2022-09-30',\n",
    "    ) \n",
    "    \n",
    "    num_results = search.matched()\n",
    "    item_collection = search.get_all_items()\n",
    "    \n",
    "    tile_name = \"T\" + current_tile\n",
    "    iter_items = 0\n",
    "    for i in tqdm(item_collection ,desc=f\"({tile_iter}/{len(tiles)})\"):\n",
    "        if i.id.split('.')[2] == tile_name:\n",
    "            if i.properties['eo:cloud_cover'] >= cloud_lower and i.properties['eo:cloud_cover'] <= cloud_upper:\n",
    "                response = requests.get(i.assets['metadata'].href)\n",
    "                if response.status_code == 200:\n",
    "                    temp_xml = response.text\n",
    "                    temp_xml = xmltodict.parse(temp_xml)\n",
    "                    temp_dict = {\"tile_id\": tile_name, \"cloud_cover\": i.properties['eo:cloud_cover'], \n",
    "                                 \"spatial_cover\": int(temp_xml['Granule']['AdditionalAttributes']['AdditionalAttribute'][3]['Values']['Value']),\n",
    "                                 \"http_links\": {\"Fmask\": i.assets['Fmask']},\n",
    "                                 \"s3_links\": {\"Fmask\": i.assets['Fmask'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/')}}\n",
    "                    tile_list.append(temp_dict)\n",
    "                    iter_items += 1\n",
    "                else: \n",
    "                    assert False, f\"Failed to fetch XML from {i.assets['metadata'].href}. Error code: {response.status_code}\"\n",
    "            \n",
    "    tile_iter += 1\n",
    "    \n",
    "tile_df = pd.DataFrame(tile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8bb4e-ff0c-4574-8ce1-6b63b80cfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv for later uses\n",
    "tile_df.to_csv(req_path + \"tile_df_fmask.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52379359-d833-4b68-b70f-30ba26da539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5049ed0-0be0-474c-a370-0c193f92bc23",
   "metadata": {},
   "source": [
    "## Filtering based on could and spatial coverage of the tiles we gathered earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51b51e-0e36-45e1-a173-b524107631e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_filtering (dataframe):\n",
    "    \"\"\"\n",
    "        Using spatial coverage percentage to filter chips\n",
    "\n",
    "        Args:\n",
    "            dataframe: A pandas dataframe that generated previously\n",
    "    \"\"\"\n",
    "    cover_list = [100, 90, 80, 70, 60, 50]\n",
    "    tile_list_ft = []\n",
    "    tile_list = dataframe.tile_id.unique().tolist()\n",
    "    \n",
    "    for tile in tqdm(tile_list):\n",
    "        temp_df = dataframe[dataframe.tile_id == tile]\n",
    "        for cover_pct in cover_list:\n",
    "            \n",
    "            temp_df_filtered = temp_df[temp_df.spatial_cover >= cover_pct]\n",
    "            if len(temp_df_filtered) >= 6: # Number of \"timestep\" wish to get for each tile\n",
    "                for i in range(len(temp_df_filtered)):\n",
    "                    tile_list_ft.append(temp_df_filtered.iloc[i])\n",
    "                break\n",
    "    \n",
    "    tile_df_filtered = pd.DataFrame(tile_list_ft)\n",
    "    return tile_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532546b0-3e08-4013-96d7-f171a76de565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_scenes(dataframe):\n",
    "    \"\"\"\n",
    "        Selecting best spatial covered scenes based on timesteps\n",
    "\n",
    "        Args:\n",
    "            dataframe: A pandas dataframe that generated previously\n",
    "    \"\"\"\n",
    "    select_tiles = []\n",
    "    tile_list = dataframe.tile_id.unique().tolist()\n",
    "\n",
    "    for tile in tqdm(tile_list):\n",
    "        temp_df = dataframe[dataframe.tile_id == tile].reset_index(drop=True)\n",
    "        for i in range(6): # Number of \"timestep\" wish to get for each tile\n",
    "            select_tiles.append(temp_df.loc[i])\n",
    "\n",
    "    return pd.DataFrame(select_tiles).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cba9d8-7129-4c1f-b104-4da96dbc4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_filter_process(dataframe):\n",
    "    sptial_filtered_df =  spatial_filtering(dataframe)\n",
    "    time_selected_df = select_scenes(sptial_filtered_df)\n",
    "    return time_selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1d6a5-2326-4ff3-9522-e4d03dd086b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_tiles = tile_filter_process(tile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bc910-e842-44f8-8440-c3b358759cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv for later uses\n",
    "selected_tiles.to_csv(req_path + \"selected_tiles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f2af2-381b-46af-8a91-c1bc3808ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ccf430-586a-4be4-9976-9c941c93c1aa",
   "metadata": {},
   "source": [
    "## Data downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879642d-6b53-4c06-bd11-24151ec4b690",
   "metadata": {},
   "source": [
    "### Creating netrc file on root for credentials (Run Once each docker session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab067f-8e85-4110-8507-5eaed64c5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "urs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\n",
    "prompts = ['Enter NASA Earthdata Login Username: ',\n",
    "           'Enter NASA Earthdata Login Password: ']\n",
    "\n",
    "# Determine the OS (Windows machines usually use an '_netrc' file)\n",
    "netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n",
    "\n",
    "# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\n",
    "try:\n",
    "    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n",
    "    netrc(netrcDir).authenticators(urs)[0]\n",
    "\n",
    "# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\n",
    "except FileNotFoundError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n",
    "    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    # Set restrictive permissions\n",
    "    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n",
    "\n",
    "    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\n",
    "except TypeError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n",
    "    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327da3f9-3240-460a-8aff-c2ffdba7a5b2",
   "metadata": {},
   "source": [
    "### Getting temporary credentials from NASA's S3 Bucket(Run once each docker session to make sure it works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ecdb5-b0ad-4b9a-a591-3535b6b7d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7277bda-b675-4bd3-8595-1e4aec642789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temp_creds():\n",
    "    temp_creds_url = s3_cred_endpoint\n",
    "    return requests.get(temp_creds_url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb3603-b94c-4414-983e-fe67ad05bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_creds_req = get_temp_creds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a78f5-cadd-4400-a677-149b10de8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n",
    "                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n",
    "                        aws_session_token=temp_creds_req['sessionToken'],\n",
    "                        region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79990622-a795-457f-9707-c620dbbec7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rio_env = rasterio.Env(AWSSession(session),\n",
    "                  GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n",
    "                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\n",
    "rio_env.__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1f1d7-1528-4e60-b31f-5f2608018666",
   "metadata": {},
   "source": [
    "### Tile downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46515d6b-35e3-4b81-a2fe-674ce261ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The download process would generate a temp credential for each tile download. \n",
    "# A multi-processed version is WIP.\n",
    "def tile_download(table, from_csv = True):\n",
    "    \"\"\"\n",
    "        Downloading tiles by reading from the metadata information gathered earlier\n",
    "\n",
    "        Args:\n",
    "            table: A pandas dataframe that generated previously\n",
    "            boto3_session: The session that set earlier when getting credentials\n",
    "            from_csv: If the tile information is from a csv, then True\n",
    "    \"\"\"\n",
    "    info_list = []\n",
    "    bands = [\"Fmask\"]\n",
    "    accept_tiles = np.unique(table.tile_id)\n",
    "    for tile in tqdm(accept_tiles):\n",
    "\n",
    "        temp_creds_req = get_temp_creds()\n",
    "        session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n",
    "                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n",
    "                        aws_session_token=temp_creds_req['sessionToken'],\n",
    "                        region_name='us-west-2')\n",
    "        \n",
    "        temp_tb = table[table.tile_id == tile]\n",
    "        for i in range(6): # Number of \"timestep\" wish to get for each tile\n",
    "            if from_csv:\n",
    "                bands_dict = json.loads(temp_tb.iloc[i].s3_links.replace(\"'\", '\"'))\n",
    "            else:\n",
    "                bands_dict = temp_tb.iloc[i].s3_links\n",
    "            for band in bands:\n",
    "                temp_key = bands_dict[band].replace(\"s3:/\", \"\")\n",
    "                temp_sav_path = f\"/data/cloudMask/tiles_fmask/{bands_dict[band].split('/')[2]}/{bands_dict[band].split('/')[3]}\"\n",
    "                os.makedirs(f\"/data/cloudMask/tiles_fmask/{bands_dict[band].split('/')[2]}\", exist_ok=True)\n",
    "                if not Path(temp_sav_path).is_file():\n",
    "                    session.resource('s3').Bucket('lp-prod-protected').download_file(Key = temp_key, Filename = temp_sav_path)\n",
    "            temp_dict = {\"tile\":tile, \"count\":i, \"save_path\":f\"/data/cloudMask/tiles_fmask/{bands_dict[band].split('/')[2]}/\", \"filename\":bands_dict[\"Fmask\"].split('/')[3].replace(\".Fmask.tif\",\"\")}\n",
    "            info_list.append(temp_dict)\n",
    "    return pd.DataFrame(info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d807452-c78b-4009-8309-9fab89e7acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df = tile_download(selected_tiles, from_csv = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e30cf-74dd-4761-8bc3-0b8d8f31d90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "track_df.to_csv(req_path + \"track_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9f780-bf20-4c2a-adf3-859bf0edcdbd",
   "metadata": {},
   "source": [
    "## Chipping (Run hls_reproject.ipynb before going into this chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8f043-5b1b-41a8-97a5-8dad69cc96fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all saved dataframes and json\n",
    "chip_df = pd.read_csv(req_path + \"chip_df.csv\")\n",
    "with open(\"/home/data/chip_ids.json\", 'r') as f:\n",
    "    chip_ids = json.load(f)\n",
    "track_df = pd.read_csv(req_path + \"track_df.csv\")\n",
    "with open(chip_file, \"r\") as file:\n",
    "    chips = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b61e60-1c57-43d1-a2c3-d6612dff614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_to_chip = track_df.tile.unique().tolist()\n",
    "with open(chip_file, \"r\") as file_chip:\n",
    "    chipping_js = json.load(file_chip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959df38-61d8-438a-b2c5-1f4489bd8d10",
   "metadata": {},
   "source": [
    "### Chipping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c525b-6f2e-4551-ae68-fd2dceca12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The qa_lut lookup table is from the nasa_hls repo.\n",
    "## For more information check \"https://benmack.github.io/nasa_hls/build/html/tutorials/Working_with_HLS_datasets_and_nasa_hls.html\"\n",
    "qa_lut = pd.read_csv(\"/home/data/qa_lut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e8e60-14f5-4c1e-9435-98ac1fb096b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cloud_classes = qa_lut[qa_lut.cloud == True].qa_value.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce69290-1634-4092-b564-3db49f9e2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_mask_reclass(x):\n",
    "    ## binary reclass of the cloud mask\n",
    "    return(cloud_classes.count(x))\n",
    "\n",
    "c_rcl = np.vectorize(cloud_mask_reclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d6b8d-c25b-451c-9889-0a5f491600bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chip(chip_id, \n",
    "                 chip_tile,\n",
    "                 shape,\n",
    "                 track_csv):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives a chip id, HLS tile, chip geometry, and a list of bands to process. \n",
    "    \n",
    "    Assumptions:\n",
    "    \n",
    "    Inputs:\n",
    "    - chip_id: string of chip id, e.g. '000_001'\n",
    "    - chip_tile: string of HLS tile , e.g. '15ABC'\n",
    "    - shape: 'geometry' property of single polygon feature read by fiona\n",
    "    \n",
    "    The function writes out a multi-date TIF containing the bands for each of the three image dates for an HLS tile. \n",
    "    The function writes out a multi-date TIF containing the QA bands of each date.\n",
    "    The function writes out a chipped version of CDL. \n",
    "    The function calls check_qa(), which makes assumptions about what QA pixels are valid.\n",
    "    The function returns the number of valid QA pixels at each date, as a tuple.\n",
    "    \n",
    "    \"\"\"\n",
    "    ## get reprojected image paths\n",
    "    tile_info_df = track_csv[track_csv.tile == chip_tile]\n",
    "    \n",
    "    selected_image_folders = tile_info_df.save_path.to_list()\n",
    "\n",
    "    all_qa = []\n",
    "                     \n",
    "    for i in range(len(selected_image_folders)):\n",
    "        all_qa.append(tile_info_df.iloc[i].save_path + f\"{tile_info_df.iloc[i].filename}.Fmask.reproject.tif\")\n",
    "\n",
    "    cloud_pct = [] \n",
    "    # bad_pct_val = []\n",
    "    qa_bands = []\n",
    "                     \n",
    "    for i in range(len(selected_image_folders)):\n",
    "        with rasterio.open(all_qa[i]) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "        out_image_binary = c_rcl(out_image).astype(np.uint8)\n",
    "        qa_bands.append(out_image_binary[0])\n",
    "        cloud_pct.append(np.count_nonzero(out_image_binary[0] == 1)/(out_image_binary.shape[1] * out_image_binary.shape[2]))\n",
    "        \n",
    "    qa_bands = np.array(qa_bands).astype(np.uint8)\n",
    "        \n",
    "    with rasterio.open(all_qa[0]) as src:\n",
    "        out_meta = src.meta\n",
    "    \n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": qa_bands.shape[1],\n",
    "                     \"width\": qa_bands.shape[2],\n",
    "                     \"count\": 1,\n",
    "                     \"transform\": out_transform})\n",
    "    for i in range(len(selected_image_folders)):\n",
    "        current_cmask = np.expand_dims(qa_bands[i], axis=0)\n",
    "        with rasterio.open(f\"{chip_fmask_dir}{str(chip_id)}_{i}_cmask.tif\", \"w\", **out_meta) as dest:\n",
    "            dest.write(current_cmask)  \n",
    "    \n",
    "    return(cloud_pct)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcae6c-5dc0-48a9-ba42-036dcb4257c7",
   "metadata": {},
   "source": [
    "### Chipping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f5f1a-a814-456e-9913-c6ccfd4335a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process chips\n",
    "fmask_chips_info = []\n",
    "for tile in tiles_to_chip:\n",
    "    chips_to_process = chip_df[chip_df.tile == tile.replace(\"T\", \"\")].reset_index(drop = True)\n",
    "    for k in range(len(chips_to_process)):\n",
    "        current_id = chips_to_process.chip_id[k]\n",
    "        chip_tile = chips_to_process.tile[k]\n",
    "        # print(current_id)\n",
    "        chip_index = chip_ids.index(current_id)\n",
    "\n",
    "        chip_feature = chipping_js['features'][chip_index]\n",
    "\n",
    "        shape = [chip_feature['geometry']]\n",
    "\n",
    "        ## do we want to scale/clip reflectances?\n",
    "        full_tile_name = \"T\" + chip_tile\n",
    "        cloud_pct = process_chip(current_id, full_tile_name, shape, track_df)\n",
    "\n",
    "        chip_df_index = chip_df.index[chip_df['chip_id'] == current_id].tolist()[0]\n",
    "\n",
    "        for i in range(len(cloud_pct)):\n",
    "            temp_dict = {\"fmask_name\": f\"{current_id}_{i}_cmask.tif\",\n",
    "                         \"cloud_pct\": cloud_pct[i]}\n",
    "            fmask_chips_info.append(temp_dict)\n",
    "fmask_tracker = pd.DataFrame(fmask_chips_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade488a2-9ba2-412b-bf31-fa553274557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmask_tracker.to_csv(\"/home/data/fmask_tracker.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
