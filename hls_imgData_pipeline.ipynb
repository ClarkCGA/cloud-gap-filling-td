{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be689edd-cbdd-4b3a-a165-ce588c21b2c7",
   "metadata": {},
   "source": [
    "# HLS image query, download and chipping pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a906205-e792-4167-9879-b0c59f4f0953",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248bdc55-5416-4c0f-b76c-6bbb5ff59df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import json\n",
    "import shapely\n",
    "import shapely.geometry\n",
    "import xarray\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import os\n",
    "import fiona\n",
    "import urllib.request as urlreq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import xmltodict\n",
    "import shutil\n",
    "import datetime\n",
    "import boto3\n",
    "import pyproj\n",
    "import time\n",
    "\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "from pystac_client import Client \n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio import Affine\n",
    "from rasterio.crs import CRS\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import Popen, PIPE\n",
    "from tqdm import tqdm\n",
    "from netrc import netrc\n",
    "from subprocess import Popen\n",
    "from platform import system\n",
    "from getpass import getpass\n",
    "from rasterio.session import AWSSession\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f97211-2dcc-45a8-bf04-255679bc328c",
   "metadata": {},
   "source": [
    "## Setting folder pathes and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3966fbc-2da6-4504-9eeb-707831a40aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START OPTIONS #####\n",
    "\n",
    "## Threshold of cloud cover percentage. \n",
    "## Five is recommended for HLS raw images since less might leave to no tiles pass the filter\n",
    "cloud_thresh = 5\n",
    "\n",
    "## Root paths\n",
    "root_path = \"/data/\" # Place to save the downloaded/reprojected/chipped image data\n",
    "req_path = \"/home/data/\" # Place to save all the csv and geojsons within the GitHub repo\n",
    "extra_files = \"/data/requirements/\" # Place to save things that should not be uploaded to GitHub\n",
    "\n",
    "## File paths\n",
    "query_file = req_path + \"chip_bbox_task_3.geojson\" # Path to the EPSG:4326 geojson\n",
    "chip_file =  req_path + \"chip_bbox_task_3_5070.geojson\" # Path to the EPSG:5070 geojson\n",
    "chip_csv = req_path + \"chip_tracker.csv\" # Path to the chip tracking csv\n",
    "kml_file = extra_files + 'sentinel_tile_grid.kml' # Path to the sentinel grid kml file\n",
    "\n",
    "## Save paths\n",
    "## Manually creating these folders is recommended before running the pipeline\n",
    "chip_dir = root_path + 'chips/' # Place to save the chipped data\n",
    "tile_dir = root_path + 'tiles/' # Place to save the downloaded/reprojected tiles\n",
    "chip_fmask_dir = root_path + 'chips_fmask/' # Place to save the chipped fmasks\n",
    "chip_dir_filt = root_path + 'chips_filtered/' # Place to save the filtered chips\n",
    "\n",
    "#####  END OPTIONS  #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e884a-d995-48c1-95c5-db1643b65a3b",
   "metadata": {},
   "source": [
    "## Read in csvs and jsons from saved file (Only run when need to load from previously query results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c069f6b-9a39-423e-880d-c05eb00a8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all saved dataframes and json\n",
    "chip_df = pd.read_csv(req_path + \"chip_df.csv\")\n",
    "with open(req_path + \"chip_ids.json\", 'r') as f:\n",
    "    chip_ids = json.load(f)\n",
    "track_df = pd.read_csv(req_path + \"track_df.csv\")\n",
    "with open(chip_file, \"r\") as file:\n",
    "    chips = json.load(file)\n",
    "selected_tiles = pd.read_csv(req_path + \"selected_tiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70661fd8-19e9-44e4-a194-efd1f323ceeb",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed51a15-e247-47cd-b6a4-977756b4178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading chips bounding boxes from geojson\n",
    "with open(query_file, \"r\") as file:\n",
    "    chips = json.load(file)\n",
    "\n",
    "# Create lists about chip information to find tiles corresponding to it later\n",
    "chip_ids = []\n",
    "chip_x = []\n",
    "chip_y = []\n",
    "\n",
    "for item in chips['features']:\n",
    "    chip_ids.append(item['properties']['id'])\n",
    "    chip_x.append(item['properties']['center'][0])\n",
    "    chip_y.append(item['properties']['center'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836e6c8-e23d-4531-aa00-333f6c3b0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the chip_ids for chipping uses\n",
    "with open(extra_files + \"chip_ids.json\", \"w\") as f:\n",
    "    json.dump(chip_ids, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6642fe-7b11-434f-b1b9-9bb1bacb3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sentinel kml file\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "tile_src = geopandas.read_file(kml_file, driver='KML')\n",
    "\n",
    "# Create table containing information about sentinel tiles\n",
    "tile_name = []\n",
    "tile_x = []\n",
    "tile_y = []\n",
    "\n",
    "for tile_ind in range(tile_src.shape[0]):\n",
    "    tile_name.append(tile_src.iloc[tile_ind].Name)\n",
    "    tile_x.append(tile_src.iloc[tile_ind].geometry.centroid.x)\n",
    "    tile_y.append(tile_src.iloc[tile_ind].geometry.centroid.y)\n",
    "\n",
    "tile_name = np.array(tile_name)\n",
    "tile_x = np.array(tile_x)\n",
    "tile_y = np.array(tile_y)\n",
    "tile_src = pd.concat([tile_src, tile_src.bounds], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b3a055-45a7-40f8-87ad-01fc7d3e95d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>geometry</th>\n",
       "      <th>minx</th>\n",
       "      <th>miny</th>\n",
       "      <th>maxx</th>\n",
       "      <th>maxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01CCV</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -7...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-73.064633</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-72.012478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01CDH</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-83.835334</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-82.796720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01CDJ</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-82.939452</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-81.906947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01CDK</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-82.044055</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-81.016439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01CDL</td>\n",
       "      <td>TILE PROPERTIES&lt;br&gt;&lt;table border=0 cellpadding...</td>\n",
       "      <td>GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-81.148070</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-80.124456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name                                        Description  \\\n",
       "0  01CCV  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "1  01CDH  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "2  01CDJ  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "3  01CDK  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "4  01CDL  TILE PROPERTIES<br><table border=0 cellpadding...   \n",
       "\n",
       "                                            geometry   minx       miny   maxx  \\\n",
       "0  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -7... -180.0 -73.064633  180.0   \n",
       "1  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8... -180.0 -83.835334  180.0   \n",
       "2  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8... -180.0 -82.939452  180.0   \n",
       "3  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8... -180.0 -82.044055  180.0   \n",
       "4  GEOMETRYCOLLECTION Z (POLYGON Z ((180.00000 -8... -180.0 -81.148070  180.0   \n",
       "\n",
       "        maxy  \n",
       "0 -72.012478  \n",
       "1 -82.796720  \n",
       "2 -81.906947  \n",
       "3 -81.016439  \n",
       "4 -80.124456  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_src.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02dc8d89-fc24-4bba-a459-f78ae8de8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that help to match the chip shape to a certain tile\n",
    "def find_tile(x,y):\n",
    "# Identify closest tile\n",
    "    s = (tile_x - x)**2+(tile_y - y)**2\n",
    "    tname = tile_name[np.argmin(s)]\n",
    "    return(tname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ec3c74-4751-4e0e-9d27-6a9d6acd3f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign each chip a tile by using the find_tile function\n",
    "chip_df = pd.DataFrame({\"chip_id\" : chip_ids, \"chip_x\" : chip_x, \"chip_y\" : chip_y})\n",
    "chip_df['tile'] = chip_df.apply(lambda row : find_tile(row['chip_x'], row['chip_y']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022b1b13-5bf7-4f59-98d0-08381820f2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>chip_x</th>\n",
       "      <th>chip_y</th>\n",
       "      <th>tile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>chip_115_314</td>\n",
       "      <td>-99.060383</td>\n",
       "      <td>44.522330</td>\n",
       "      <td>14TMQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>chip_148_412</td>\n",
       "      <td>-90.907704</td>\n",
       "      <td>42.470597</td>\n",
       "      <td>15TXH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>chip_081_204</td>\n",
       "      <td>-108.698883</td>\n",
       "      <td>46.021321</td>\n",
       "      <td>12TXS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>chip_237_443</td>\n",
       "      <td>-88.910720</td>\n",
       "      <td>37.022803</td>\n",
       "      <td>16SCG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>chip_109_315</td>\n",
       "      <td>-98.991689</td>\n",
       "      <td>44.886159</td>\n",
       "      <td>14TMQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           chip_id      chip_x     chip_y   tile\n",
       "9995  chip_115_314  -99.060383  44.522330  14TMQ\n",
       "9996  chip_148_412  -90.907704  42.470597  15TXH\n",
       "9997  chip_081_204 -108.698883  46.021321  12TXS\n",
       "9998  chip_237_443  -88.910720  37.022803  16SCG\n",
       "9999  chip_109_315  -98.991689  44.886159  14TMQ"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chip_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e225b-501a-4775-8a4f-5d10a4e04f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv for later uses\n",
    "chip_df.to_csv(req_path + \"chip_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4833df03-02b7-40da-8257-d8419fb340b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13TDE', '16SDD', '13SFV', '14TNS', '14UMU']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiles = chip_df.tile.unique().tolist()\n",
    "tiles[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caf2ff4f-922c-4b5f-a1ab-1f59fd9ebc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7af0f-1c99-47c4-ae59-c1e0f72cf0f1",
   "metadata": {},
   "source": [
    "## Querying tile links based on geometry of chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b81d838-473d-46b0-ac25-cefae3cabb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the API and open it in the client\n",
    "STAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "catalog = Client.open(f'{STAC_URL}/LPCLOUD/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c80adeec-2bea-4796-8b2b-7110e9f059bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_meta_to_df(tiles, chip_df, chip_ids, cloud_thresh):\n",
    "    tile_list = []\n",
    "    failed_list = []\n",
    "    print(f\"There are a total of {len(tiles)} tiles\")\n",
    "    for current_tile in tqdm(tiles):\n",
    "    \n",
    "        tile_name = \"T\" + current_tile\n",
    "        iter_items = 0\n",
    "        \n",
    "        chip_df_filt = chip_df.loc[chip_df.tile == current_tile]#.reset_index()\n",
    "        first_chip_id = chip_df_filt.chip_id.iloc[0]\n",
    "        first_chip_index_in_json = chip_ids.index(first_chip_id)\n",
    "        roi = chips['features'][first_chip_index_in_json]['geometry']\n",
    "    \n",
    "        search = catalog.search(\n",
    "            collections = ['HLSS30.v2.0'],\n",
    "            intersects = roi,\n",
    "            datetime = '2022-03-01/2022-09-30',\n",
    "        )\n",
    "        \n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                num_results = search.matched()\n",
    "            except Exception:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"After {attempt} retry problem presists for tile {tile_name} with STAC-API error, check your internet connection, the geojson file and the status of NASA STAC-API and try again. Continue to next tile\")\n",
    "            failed_list.append(tile_name)\n",
    "            continue\n",
    "        \n",
    "        item_collection = search.get_all_items()\n",
    "    \n",
    "        for i in item_collection:\n",
    "            respond_code = 0\n",
    "            for attempt in range(5): \n",
    "                if i.id.split('.')[2] == tile_name:\n",
    "                    if i.properties['eo:cloud_cover'] <= cloud_thresh:\n",
    "                        response = requests.get(i.assets['metadata'].href)\n",
    "                        respond_code = response.status_code\n",
    "                        if response.status_code == 200:\n",
    "                            temp_xml = response.text\n",
    "                            temp_xml = xmltodict.parse(temp_xml)\n",
    "                            temp_dict = {\"tile_id\": tile_name, \"cloud_cover\": i.properties['eo:cloud_cover'],\n",
    "                                         \"date\": datetime.datetime.strptime(i.properties['datetime'].split('T')[0], \"%Y-%m-%d\"), \n",
    "                                         \"spatial_cover\": int(temp_xml['Granule']['AdditionalAttributes']['AdditionalAttribute'][3]['Values']['Value']),\n",
    "                                         \"http_links\": {\"B02\": i.assets['B02'].href, \"B03\": i.assets['B03'].href, \"B04\": i.assets['B04'].href,  \"B8A\": i.assets['B8A'].href,\n",
    "                                                        \"B11\": i.assets['B11'].href, \"B12\": i.assets['B12'].href, \"Fmask\": i.assets['Fmask']},\n",
    "                                        \"s3_links\": {\"B02\": i.assets['B02'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'), \n",
    "                                                     \"B03\": i.assets['B03'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'), \n",
    "                                                     \"B04\": i.assets['B04'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'), \n",
    "                                                     \"B8A\": i.assets['B8A'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'),\n",
    "                                                     \"B11\": i.assets['B11'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'),\n",
    "                                                     \"B12\": i.assets['B12'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/'),\n",
    "                                                     \"Fmask\": i.assets['Fmask'].href.replace('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/', 's3:/')}}\n",
    "                            tile_list.append(temp_dict)\n",
    "                            break\n",
    "                        else:\n",
    "                            time.sleep(1)\n",
    "            if attempt == 5 and respond_code != 200:\n",
    "                print(f\"After {attempt} retry problem presists for tile {tile_name}, failed to fetch XML data from {i.assets['metadata'].href}, error code {respond_code}, continue to next tile.\")\n",
    "                failed_list.append(tile_name)\n",
    "    \n",
    "    return pd.DataFrame(tile_list), failed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f83c5e98-0297-4d80-a464-b284357b79fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 5 tiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:56<00:00, 11.39s/it]\n"
     ]
    }
   ],
   "source": [
    "tile_df, failed_list = get_meta_to_df(tiles[0:5], chip_df, chip_ids, cloud_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8bb4e-ff0c-4574-8ce1-6b63b80cfabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv for later uses\n",
    "tile_df.to_csv(req_path + \"tile_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52379359-d833-4b68-b70f-30ba26da539b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tile_id</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>date</th>\n",
       "      <th>spatial_cover</th>\n",
       "      <th>http_links</th>\n",
       "      <th>s3_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T13TDE</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>25</td>\n",
       "      <td>{'B02': 'https://data.lpdaac.earthdatacloud.na...</td>\n",
       "      <td>{'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T13TDE</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-04-08</td>\n",
       "      <td>25</td>\n",
       "      <td>{'B02': 'https://data.lpdaac.earthdatacloud.na...</td>\n",
       "      <td>{'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T13TDE</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-04-18</td>\n",
       "      <td>25</td>\n",
       "      <td>{'B02': 'https://data.lpdaac.earthdatacloud.na...</td>\n",
       "      <td>{'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T13TDE</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>99</td>\n",
       "      <td>{'B02': 'https://data.lpdaac.earthdatacloud.na...</td>\n",
       "      <td>{'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T13TDE</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>99</td>\n",
       "      <td>{'B02': 'https://data.lpdaac.earthdatacloud.na...</td>\n",
       "      <td>{'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022146...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tile_id  cloud_cover       date  spatial_cover  \\\n",
       "0  T13TDE            1 2022-03-14             25   \n",
       "1  T13TDE            0 2022-04-08             25   \n",
       "2  T13TDE            0 2022-04-18             25   \n",
       "3  T13TDE            3 2022-04-21             99   \n",
       "4  T13TDE            2 2022-05-26             99   \n",
       "\n",
       "                                          http_links  \\\n",
       "0  {'B02': 'https://data.lpdaac.earthdatacloud.na...   \n",
       "1  {'B02': 'https://data.lpdaac.earthdatacloud.na...   \n",
       "2  {'B02': 'https://data.lpdaac.earthdatacloud.na...   \n",
       "3  {'B02': 'https://data.lpdaac.earthdatacloud.na...   \n",
       "4  {'B02': 'https://data.lpdaac.earthdatacloud.na...   \n",
       "\n",
       "                                            s3_links  \n",
       "0  {'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022073...  \n",
       "1  {'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022098...  \n",
       "2  {'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022108...  \n",
       "3  {'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022111...  \n",
       "4  {'B02': 's3:/HLSS30.020/HLS.S30.T13TDE.2022146...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5049ed0-0be0-474c-a370-0c193f92bc23",
   "metadata": {},
   "source": [
    "## Filtering based on could and spatial coverage of the tiles we gathered earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51b51e-0e36-45e1-a173-b524107631e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_filtering (dataframe):\n",
    "    \"\"\"\n",
    "        Using spatial coverage percentage to filter chips\n",
    "\n",
    "        Args:\n",
    "            dataframe: A pandas dataframe that generated from the query above\n",
    "    \"\"\"\n",
    "    cover_list = [100, 90, 80, 70, 60, 50]\n",
    "    tile_list_ft = []\n",
    "    tile_list = dataframe.tile_id.unique().tolist()\n",
    "    \n",
    "    for tile in tqdm(tile_list):\n",
    "        temp_df = dataframe[dataframe.tile_id == tile]\n",
    "        for cover_pct in cover_list:\n",
    "            \n",
    "            temp_df_filtered = temp_df[temp_df.spatial_cover >= cover_pct]\n",
    "            if len(temp_df_filtered) >= 3: # Number of timestep wish to get for each tile\n",
    "                for i in range(len(temp_df_filtered)):\n",
    "                    tile_list_ft.append(temp_df_filtered.iloc[i])\n",
    "                break\n",
    "    \n",
    "    tile_df_filtered = pd.DataFrame(tile_list_ft)\n",
    "    return tile_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532546b0-3e08-4013-96d7-f171a76de565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_scenes(dataframe):\n",
    "    \"\"\"\n",
    "        Selecting best spatial covered scenes based on timesteps\n",
    "\n",
    "        Args:\n",
    "            dataframe: A pandas dataframe that generated by the spatial_filtering function\n",
    "    \"\"\"\n",
    "    select_tiles = []\n",
    "    tile_list = dataframe.tile_id.unique().tolist()\n",
    "\n",
    "    for tile in tqdm(tile_list):\n",
    "        temp_df = dataframe[dataframe.tile_id == tile].sort_values('date').reset_index(drop=True)\n",
    "        select_tiles.extend([temp_df.iloc[0], temp_df.iloc[len(temp_df) // 2], temp_df.iloc[-1]])\n",
    "\n",
    "    return pd.DataFrame(select_tiles).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15742df-9ea7-4a11-aa58-417dc0187266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_filter_process(dataframe):\n",
    "    sptial_filtered_df =  spatial_filtering(dataframe)\n",
    "    time_selected_df = select_scenes(sptial_filtered_df)\n",
    "    return time_selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1d6a5-2326-4ff3-9522-e4d03dd086b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_tiles = tile_filter_process(tile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f2af2-381b-46af-8a91-c1bc3808ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bc910-e842-44f8-8440-c3b358759cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv for later uses\n",
    "selected_tiles.to_csv(req_path + \"selected_tiles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ccf430-586a-4be4-9976-9c941c93c1aa",
   "metadata": {},
   "source": [
    "## Data downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879642d-6b53-4c06-bd11-24151ec4b690",
   "metadata": {},
   "source": [
    "### Creating netrc file on root for credentials (Run Once each docker session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab067f-8e85-4110-8507-5eaed64c5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "urs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\n",
    "prompts = ['Enter NASA Earthdata Login Username: ',\n",
    "           'Enter NASA Earthdata Login Password: ']\n",
    "\n",
    "# Determine the OS (Windows machines usually use an '_netrc' file)\n",
    "netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n",
    "\n",
    "# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\n",
    "try:\n",
    "    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n",
    "    netrc(netrcDir).authenticators(urs)[0]\n",
    "\n",
    "# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\n",
    "except FileNotFoundError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n",
    "    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    # Set restrictive permissions\n",
    "    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n",
    "\n",
    "    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\n",
    "except TypeError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n",
    "    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327da3f9-3240-460a-8aff-c2ffdba7a5b2",
   "metadata": {},
   "source": [
    "### Getting temporary credentials from NASA's S3 Bucket(Run once each docker session to make sure it works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ecdb5-b0ad-4b9a-a591-3535b6b7d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint for NASA's s3 temp credential\n",
    "s3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7277bda-b675-4bd3-8595-1e4aec642789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temp_creds():\n",
    "    temp_creds_url = s3_cred_endpoint\n",
    "    return requests.get(temp_creds_url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb3603-b94c-4414-983e-fe67ad05bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_creds_req = get_temp_creds()\n",
    "#temp_creds_req                      # !!! BEWARE, removing the # on this line will print your temporary S3 credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a78f5-cadd-4400-a677-149b10de8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n",
    "                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n",
    "                        aws_session_token=temp_creds_req['sessionToken'],\n",
    "                        region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79990622-a795-457f-9707-c620dbbec7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rio_env = rasterio.Env(AWSSession(session),\n",
    "                  GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n",
    "                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\n",
    "rio_env.__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1f1d7-1528-4e60-b31f-5f2608018666",
   "metadata": {},
   "source": [
    "### Tile downloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46515d6b-35e3-4b81-a2fe-674ce261ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The download process would generate a temp credential for each tile download. \n",
    "## A multi-processed version is WIP.\n",
    "def tile_download(table, from_csv = True):\n",
    "    \"\"\"\n",
    "        Downloading tiles by reading from the metadata information gathered earlier\n",
    "\n",
    "        Args:\n",
    "            table: A pandas dataframe that generated previously\n",
    "            from_csv: If the tile information is from a csv, then uses json.loads to read the dict strctured information from csv\n",
    "    \"\"\"\n",
    "    info_list = []\n",
    "    bands = [\"B02\",\"B03\",\"B04\",\"B8A\",\"B11\",\"B12\",\"Fmask\"]\n",
    "    accept_tiles = np.unique(table.tile_id)\n",
    "    for tile in tqdm(accept_tiles):\n",
    "\n",
    "        temp_creds_req = get_temp_creds()\n",
    "        session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n",
    "                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n",
    "                        aws_session_token=temp_creds_req['sessionToken'],\n",
    "                        region_name='us-west-2')\n",
    "        \n",
    "        temp_tb = table[table.tile_id == tile]\n",
    "        for i in range(3): # Number of timestep wish to get for each tile\n",
    "            if from_csv:\n",
    "                bands_dict = json.loads(temp_tb.iloc[i].s3_links.replace(\"'\", '\"'))\n",
    "            else:\n",
    "                bands_dict = temp_tb.iloc[i].s3_links\n",
    "            for band in bands:\n",
    "                temp_key = bands_dict[band].replace(\"s3:/\", \"\")\n",
    "                temp_sav_path = f\"/data/tiles/{bands_dict[band].split('/')[2]}/{bands_dict[band].split('/')[3]}\"\n",
    "                os.makedirs(f\"/data/tiles/{bands_dict[band].split('/')[2]}\", exist_ok=True)\n",
    "                if not Path(temp_sav_path).is_file():\n",
    "                    session.resource('s3').Bucket('lp-prod-protected').download_file(Key = temp_key, Filename = temp_sav_path)\n",
    "            temp_dict = {\"tile\":tile, \"timestep\":i, \"date\":temp_tb.iloc[i].date, \"save_path\":f\"/data/tiles/{bands_dict[band].split('/')[2]}/\", \"filename\":bands_dict[\"B02\"].split('/')[3].replace(\".B02.tif\",\"\")}\n",
    "            info_list.append(temp_dict)\n",
    "    return pd.DataFrame(info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d807452-c78b-4009-8309-9fab89e7acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the process is run without loading the \"selected tiles\" from a saved CSV, then put from_csv = False\n",
    "track_df = tile_download(selected_tiles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e30cf-74dd-4761-8bc3-0b8d8f31d90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "track_df.to_csv(req_path + \"track_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9f780-bf20-4c2a-adf3-859bf0edcdbd",
   "metadata": {},
   "source": [
    "## Chipping (Run hls_reprojecting.ipynb before going into following chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8f043-5b1b-41a8-97a5-8dad69cc96fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting all saved dataframes and json\n",
    "## Only run when continuing the process from a docker restart/When not running the pipeline at once\n",
    "chip_df = pd.read_csv(req_path + \"chip_df.csv\")\n",
    "with open(req_path + \"chip_ids.json\", 'r') as f:\n",
    "    chip_ids = json.load(f)\n",
    "track_df = pd.read_csv(req_path + \"track_df.csv\")\n",
    "with open(chip_file, \"r\") as file:\n",
    "    chips = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b61e60-1c57-43d1-a2c3-d6612dff614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json for chipping and extract the list of tiles.\n",
    "tiles_to_chip = track_df.tile.unique().tolist()\n",
    "with open(chip_file, \"r\") as file_chip:\n",
    "    chipping_js = json.load(file_chip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959df38-61d8-438a-b2c5-1f4489bd8d10",
   "metadata": {},
   "source": [
    "### Chipping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eee5bc-d9e5-4c52-9510-96cc634ebd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_qa(qa_path, shape,  valid_qa = [0, 4, 32, 36, 64, 68, 96, 100, 128, 132, 160, 164, 192, 196, 224, 228]):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives a path to a qa file, and a geometry. It clips the QA file to the geometry. \n",
    "    It returns the number of valid QA pixels in the geometry, and the clipped values.\n",
    "    \n",
    "    Assumptions: The valid_qa values are taken from Ben Mack's post:\n",
    "    https://benmack.github.io/nasa_hls/build/html/tutorials/Working_with_HLS_datasets_and_nasa_hls.html\n",
    "    \n",
    "    Inputs:\n",
    "    - qa_path: full path to reprojected QA tif file\n",
    "    - shape: 'geometry' property of single polygon feature read by fiona\n",
    "    - valid_qa: list of integer values that are 'valid' for QA band.\n",
    "    \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    with rasterio.open(qa_path) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "        # print(out_image[0].shape)\n",
    "        vals = out_image.flatten()\n",
    "        unique, counts = np.unique(vals, return_counts=True)\n",
    "        qa_df = pd.DataFrame({\"qa_val\" : unique, \"counts\" : counts})\n",
    "        qa_df\n",
    "        qa_df[~ qa_df.qa_val.isin(valid_qa)].sort_values(['counts'], ascending = False)\n",
    "        qa_df['pct'] = (100 *qa_df['counts'])/(224.0 * 224.0)\n",
    "        \n",
    "        bad_qa = qa_df[~ qa_df.qa_val.isin(valid_qa)].sort_values(['counts'], ascending = False)\n",
    "        if len(bad_qa) > 0:\n",
    "            highest_invalid_percent = bad_qa.pct.tolist()[0]\n",
    "        else: \n",
    "            highest_invalid_percent = 0\n",
    "        # ncell = len(vals)\n",
    "        valid_count = sum(x in valid_qa for x in vals)\n",
    "        return(valid_count, highest_invalid_percent, out_image[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d6b8d-c25b-451c-9889-0a5f491600bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chip(chip_id, \n",
    "                 chip_tile,\n",
    "                 shape,\n",
    "                 track_csv,\n",
    "                 bands = [\"B02\", \"B03\", \"B04\", \"B8A\", \"B11\", \"B12\"]):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function receives a chip id, HLS tile, chip geometry, and a list of bands to process. \n",
    "    \n",
    "    Inputs:\n",
    "    - chip_id: string of chip id, e.g. '000_001'\n",
    "    - chip_tile: string of HLS tile , e.g. '15ABC'\n",
    "    - shape: 'geometry' property of single polygon feature read by fiona\n",
    "    - track_csv: The csv that contains path and other information. Generated earlier in the pipeline\n",
    "    - bands: The bands that wish to be included in the HLS image chips\n",
    "    \n",
    "    The function writes out a multi-date TIF containing the bands for each of the three image dates for an HLS tile. \n",
    "    The function writes out a multi-date TIF containing the QA bands of each date.\n",
    "    The function calls check_qa(), which makes assumptions about what QA pixels are valid.\n",
    "    The function returns the number of valid QA pixels at each date for chip quality control purposes.\n",
    "    \n",
    "    \"\"\"\n",
    "    ## get reprojected image paths from tracking csv\n",
    "    tile_info_df = track_csv[track_csv.tile == chip_tile]\n",
    "    selected_image_folders = tile_info_df.save_path.to_list()\n",
    "\n",
    "    # Check if each tile contains 3 timesteps\n",
    "    assert len(selected_image_folders) == 3\n",
    "    \n",
    "    # Gather date information\n",
    "    first_image_date = tile_info_df.iloc[0].date\n",
    "    second_image_date = tile_info_df.iloc[1].date\n",
    "    third_image_date = tile_info_df.iloc[2].date\n",
    "    \n",
    "\n",
    "    all_date_images = []\n",
    "    all_date_qa = []\n",
    "                     \n",
    "    for i in range(3):\n",
    "        for band in bands:\n",
    "            all_date_images.append(tile_info_df.iloc[i].save_path + f\"{tile_info_df.iloc[i].filename}.{band}.reproject.tif\")\n",
    "        all_date_qa.append(tile_info_df.iloc[i].save_path + f\"{tile_info_df.iloc[i].filename}.Fmask.reproject.tif\")\n",
    "\n",
    "    valid_first, bad_pct_first, qa_first = check_qa(all_date_qa[0], shape)\n",
    "    valid_second, bad_pct_second, qa_second = check_qa(all_date_qa[1], shape)\n",
    "    valid_third, bad_pct_third, qa_third = check_qa(all_date_qa[2], shape)\n",
    "    \n",
    "    qa_bands = []\n",
    "    qa_bands.append(qa_first)\n",
    "    qa_bands.append(qa_second)\n",
    "    qa_bands.append(qa_third)\n",
    "    qa_bands = np.array(qa_bands).astype(np.uint8)\n",
    "    \n",
    "    # Check if each image contains timestep * band_count bands\n",
    "    assert len(all_date_images) == 3 * len(bands)\n",
    "    \n",
    "    out_bands = []\n",
    "    for img in all_date_images:\n",
    "        with rasterio.open(img) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, shape, crop=True)\n",
    "            out_meta = src.meta\n",
    "            out_bands.append(out_image[0])\n",
    "    # Save to a numpy arraay\n",
    "    out_bands = np.array(out_bands)\n",
    "    # Update metadata for writing chips\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_bands.shape[1],\n",
    "                     \"width\": out_bands.shape[2],\n",
    "                     \"count\": out_bands.shape[0],\n",
    "                     \"transform\": out_transform})\n",
    "    \n",
    "    # get NA count for HLS\n",
    "    na_count = sum(out_bands.flatten() == -1000)\n",
    "    \n",
    "    # reclass negative HLS values to 0\n",
    "    out_bands = np.clip(out_bands, 0, None)\n",
    "                     \n",
    "    # write HLS chips to chip_dir\n",
    "    with rasterio.open(chip_dir + str(chip_id) + \"_merged.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(out_bands)\n",
    "\n",
    "\n",
    "        \n",
    "    ## Update metadata for QA bands chips\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": qa_bands.shape[1],\n",
    "                     \"width\": qa_bands.shape[2],\n",
    "                     \"count\": qa_bands.shape[0],\n",
    "                     \"transform\": out_transform})\n",
    "    # Write HLS chips to chip_fmask_dir\n",
    "    with rasterio.open(chip_fmask_dir + str(chip_id) + \"_Fmask.tif\", \"w\", **out_meta) as dest:\n",
    "        dest.write(qa_bands)  \n",
    "\n",
    "    return(valid_first,\n",
    "           valid_second,\n",
    "           valid_third, \n",
    "           bad_pct_first,\n",
    "           bad_pct_second,\n",
    "           bad_pct_third,\n",
    "           na_count,\n",
    "           first_image_date,\n",
    "           second_image_date,\n",
    "           third_image_date)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcae6c-5dc0-48a9-ba42-036dcb4257c7",
   "metadata": {},
   "source": [
    "### Chipping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f5f1a-a814-456e-9913-c6ccfd4335a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process chips\n",
    "## The failed_tiles is to exclude tiles that might be damaged during the reprojecting process\n",
    "failed_tiles = []\n",
    "\n",
    "for tile in tqdm(tiles_to_chip):\n",
    "    chips_to_process = chip_df[chip_df.tile == tile[1:]].reset_index(drop = True)\n",
    "    \n",
    "    for k in range(len(chips_to_process)):\n",
    "        current_id = chips_to_process.chip_id[k]\n",
    "        chip_tile = chips_to_process.tile[k]\n",
    "        chip_index = chip_ids.index(current_id)\n",
    "        chip_feature = chipping_js['features'][chip_index]\n",
    "        shape = [chip_feature['geometry']]\n",
    "        full_tile_name = \"T\" + chip_tile\n",
    "        \n",
    "        ## do we want to scale/clip reflectances?\n",
    "        try:\n",
    "            valid_first, valid_second, valid_third, bad_pct_first, bad_pct_second, bad_pct_third, na_count, first_image_date, second_image_date, third_image_date = process_chip(current_id, full_tile_name, shape, track_df)\n",
    "        except:\n",
    "            failed_tiles.append(tile)\n",
    "            break\n",
    "        \n",
    "        chip_df_index = chip_df.index[chip_df['chip_id'] == current_id].tolist()[0]\n",
    "        chip_df.at[chip_df_index, 'valid_first'] = valid_first\n",
    "        chip_df.at[chip_df_index, 'valid_second'] = valid_second\n",
    "        chip_df.at[chip_df_index, 'valid_third'] = valid_third\n",
    "        chip_df.at[chip_df_index, 'bad_pct_first'] = bad_pct_first\n",
    "        chip_df.at[chip_df_index, 'bad_pct_second'] = bad_pct_second\n",
    "        chip_df.at[chip_df_index, 'bad_pct_third'] = bad_pct_third\n",
    "        chip_df.at[chip_df_index, 'first_image_date'] = first_image_date\n",
    "        chip_df.at[chip_df_index, 'second_image_date'] = second_image_date\n",
    "        chip_df.at[chip_df_index, 'third_image_date'] = third_image_date\n",
    "        chip_df['bad_pct_max'] = chip_df[['bad_pct_first', 'bad_pct_second', 'bad_pct_third']].max(axis=1)\n",
    "        chip_df.at[chip_df_index, 'na_count'] = na_count\n",
    "\n",
    "chip_df.to_csv(req_path + \"final_chip_tracker.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfab755-6cf8-46d0-b0b7-faaa194b7762",
   "metadata": {},
   "source": [
    "### Selecting chips based on the na_count and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f38a28-7f30-4e5f-91fa-db8e20efefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chip_df =pd.read_csv(req_path + \"final_chip_tracker.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de028c8e-f1ec-47e8-b4e9-8abf2e752f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9f0db-e159-4ab4-b424-853a977494a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_chips_df = final_chip_df[final_chip_df.na_count.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07aff9-5634-4d86-aa7d-4a2c626a8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_chips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df326251-7976-41c2-91ff-f95513e9b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_chips = valid_chips_df[(valid_chips_df.bad_pct_max < 5) & (valid_chips_df.na_count == 0)].chip_id.tolist()\n",
    "for chip_id in tqdm(filtered_chips):\n",
    "    chip_paths = glob('/data/chips/*' + chip_id + '*') # Path where you store all the chips\n",
    "    for path in chip_paths:\n",
    "        name = path.split('/')[-1]\n",
    "        shutil.copyfile(path, chip_dir_filt + name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
